**部署LVS-NAT集群
#给虚拟机服务器配置网关(web,mysql,...)
]# nmcli connection modify eth1 ipv4.method manual ipv4.gateway 192.168.3.251
]# nmcli connection up eth1
#关闭eth0网卡(web,mysql,...)
]# nmcli connection down eth0
#打开路由功能(lvs)
vim /proc/sys/net/ipv4/ip_forward
1
#部署LVS-NAT集群重点注意事项
eg:web1和web2必须配置网关地址，为linux lvs server的eth1网址

**部署LVS-DR集群
#配置linux lvs server的eth0:0static网址:192.168.5.10
]# vim /etc/sysconfig/network-scripts/ifcfg-eth0:0
# Generated by dracut initrd
DEVICE=eth0:0
NAME=eth0:0
ONBOOT=yes
IPV6INIT=no
BOOTPROTO=none
IPADDR=192.168.5.10
PREFIX=24
TYPE=Ethernet
#] systemctl restart network
#] systemctl enable network
*配置web服务虚拟机的ifcfg-lo:0的配置：192.168.5.10
#这里的子网掩码必须是32(也就是255),网址地址与ip地址一样，广播地址与ip地址也一样
]# vim /etc/sysconfig/network-scripts/ifcfg-lo:0
DEVICE=lo:0
IPADDR=192.168.5.10
NETMASK=255.255.255.255
NETWORK=192.168.5.10
# If you're having problems with gated making 127.0.0.0/8 a martian,
# you can change this to something else (255.255.255.255, for example)
BROADCAST=196.168.5.10
ONBOOT=yes
NAME=lo:0
*防止地址冲突,在/etc/sysctl.conf添加如下4行内容
]# vim /etc/sysctl.conf
net.ipv4.conf.all.arp_ignore=1
net.ipv4.conf.lo.arp_ignore=1
net.ipv4.conf.all.arp_announce=2
net.ipv4.conf.lo.arp_announce=2
]# sysctl -p	//让配置生效
]# systemctl restart network	//重启配置让网络生效
]# systemctl enable network
*在linux lvs server上添加规则:端口为80的web服务
注释:规则ip为eth0:0与lo:0的ip
]# ipvsadm -A -t 192.168.5.10:80
]# ip ipvsadm -a -t 192.168.5.10:80 -r 192.168.5.2:80 -w 1 -g
]# ip ipvsadm -a -t 192.168.5.10:80 -r 192.168.5.8:80 -w 1 -g
]# ip ipvsadm-save -n > /etc/sysconfig/ipvsadm
]# ip ipvsadm -Ln
*客户端(client)测试
]# curl http://192.168.5.10
**动态检测lvs健康检查功能的脚本
]# vim lvs_health_examination.sh
#!/bin/bash
VIP=192.168.5.10:80
RIP1=192.168.5.2
RIP2=192.168.5.8
for IP in $RIP1 $RIP2
do
  curl -s http://$IP &> /dev/null
  if [ $? -eq 0 ];then
    ipvsadm -Ln | grep -q $IP || ipvsadm -a -t $VIP -r $IP
  else
    ipvsadm -Ln | grep -q $IP && ipvsadm -d -t $VIP -r $IP
  fi
done
sleep 1

**搭建部署keepalived高可用服务
*ansible部署keepalived包
[root@agent ~]# ansible user -m yum -a 'name=keepalived state=installed'
*weib服务器(user1,user2)配置/etc/keepalived/keepalived.conf文件
]# vim /etc/keepalived/keepalived.conf
! Configuration File for keepalived

global_defs {
   notification_email {
     acassen@firewall.loc	//设置报警收件人邮箱
     failover@firewall.loc	//设置报警收件人邮箱
     sysadmin@firewall.loc	//设置报警收件人邮箱
   }
   notification_email_from Alexandre.Cassen@firewall.loc  //设置发件人
   smtp_server 192.168.200.1	//定义邮件服务器
   smtp_connect_timeout 30
   router_id user1		//设置路由id号
   vrrp_skip_check_adv_addr
   vrrp_strict
   vrrp_garp_interval 0
   vrrp_gna_interval 0
}

vrrp_instance VI_1 {
    state MASTER	//主服务器为MASTER(备服务器需要修改为BACKUP)
    interface eth0	//定义网络借口
    virtual_router_id 51	//主备服务器VRID号必须一致
    priority 100	//服务器优先级，优先级高优先获取VIP
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111	//主备服务器密码必须一致
    }
    virtual_ipaddress {
        192.168.5.80	//谁是主服务器谁获得该IP
    }
}
*启动keepalived服务
]# systemctl start keepalived
]# systemctl enable keepalived
*启动keepalived会自动添加一个drop的防火墙规则，需要清空
]# iptables -F
*测试web两台服务器查看VIP信息
]# ip a s eth0	//只有master机器才会有VIP:192.168.5.80，当master机器down掉时，会自动启用slave机器为master机器，自动获得VIP:192.168.5.80
*客户端访问http://192.168.5.80
client ~]# curl http://192.168.5.80	//始终访问的时master机器的web服务


**搭建部署lvs调度器+keepalived高可用服务
*1.保证agent组的两台lvs调度其[proxy,better]没有eth0:0网卡,eth0的VIP由keepaliveed配置
*2.保证user组的两台web服务器关闭keepalived服务
proxy|better ]# yum -y install ipvsadm
proxy|better ]# ipvsadm -C
proxy|better ]# yum -y install keepalived
proxy|better ]# vim /etc/keepalived/keepalived.conf
! Configuration File for keepalived

global_defs {
   notification_email {
     acassen@firewall.loc
     failover@firewall.loc
     sysadmin@firewall.loc
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS1|LVS2	//设置路由ID号
   vrrp_skip_check_adv_addr
   vrrp_strict
   vrrp_garp_interval 0
   vrrp_gna_interval 0
}

vrrp_instance VI_1 {
    state MASTER|BACKUP	//主服务器为MASTER|BACKUP
    interface eth0
    virtual_router_id 50
    priority 100|50	//服务器优先级
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.5.10	//配置VIP
    }
}

virtual_server 192.168.5.10 80 {	//设置ipvsadm的VIP规则
    delay_loop 6
    lb_algo wrr		//设置LVS调度算法为wrr
    lb_kind DR		//设置LVS的模式为DR
    #persistence_timeout 50	//注释掉可以保持连接，开启后，客户端在一定时间内始终访问相同服务
    protocol TCP

    real_server 192.168.5.8 80 {	//设置后端web服务器真实IP
        weight 1	//设置权重为1
        TCP_CHECK {	//对后台real_server作健康检查
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
         }
    }

    real_server 192.168.5.2 80 {
        weight 2
        TCP_CHECK {
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
         }
    }
}
*开启keepalived服务
proxy|better ]# systemctl start keepalived.service
proxy|better ]# systemctl enable keepalived.service
proxy|better ]# iptables -F	//清楚keepalivd启动后的drop规则，务必要执行此操作
*查看LVS规则
proxy|better ]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.5.10:80 wrr
  -> 192.168.5.2:80               Route   2      0          2         
  -> 192.168.5.8:80               Route   1      0          0
*查看VIP配置
proxy|better ]# ip a s eth0
#出现如下相关字样，表示成功，有时可能需要等待一段时间
... ...
inet 192.168.5.10/32 scope global eth0
... ...
*客户端测试
client ~]# curl http://192.168.5.10

**配置HAProxy负载平衡集群
*清理lvs调度器部署
#删除所有设备的VIP，清空所有LVS设置，关闭keepalived
proxy ~]# systemctl stop keepalived.service
proxy ~]# systemctl disable keepalived.service
proxy ~]# ipvsadm -C
proxy ~]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
user1|user2 ~]# systemctl stop keepalived.service
user1|user2 ~]# systemctl disable keepalived.service
user2 ~]# rm -rf /etc/sysconfig/network-scripts/ifcfg-lo:0
user1|user2 ~]# ifdown lo:0
user1|user2 ~]# rm -rf /etc/sysconfig/network-scripts/ifcfg-lo:0
user1|user2 ~]# systemctl restart network
*测试访问web服务3.222与3.244的网页
proxy ~]# curl http://192.168.3.244
proxy ~]# curl http://192.168.3.222
*安装软件
agent ~]# ansible proxy -m yum -a 'name=haproxy state=installed'
*配置
proxy ~]# vim /etc/haproxy/haproxy.cfg
#---------------------------------------------------------------------
# main frontend which proxys to the backends
#---------------------------------------------------------------------
#增加如下4行内容
listen webs 0.0.0.0.:80
  balance roundrobin
  server user1 192.168.3.222:80
  server user2 192.168.3.244:80 check inter 2000 rise 2 fall 2
proxy ~]# systemctl start haproxy.service
proxy ~]# systemctl enable haproxy.service
proxy ~]# ss -anpltu | grep :80
tcp    LISTEN     0      128       *:80                    *:*                   users:(("haproxy",pid=22620,fd=5))
*客户端测试web服务
client ~]# curl http://192.168.5.7
*查看监控信息
proxy ~]# vim /etc/haproxy/haproxy.cfg
#增加如下内容
listen stats 0.0.0.0:1080
  stats uri /stats
  stats refresh 30s
  stats realm Haproxy Manager
  stats auth admin:admin
proxy ~]# systemctl restart haproxy.service
注释:firefox浏览器打开网址192.168.3.251:1080/stats,将会查看到web服务机器的状态信息，因此生产环境没有把web服务机器的网关指向调度器proxy,故只能用proxy的eth1网卡ip打开，如果正确配置好网关，则的打开的网址为192.168.5.7:1080/stats

**修改配置详解:

global
    log         127.0.0.1 local2	//[err warning info debug]
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid	//haproxy的pid存放路径
    maxconn     4000			//最大连接数，默认4000
    user        haproxy
    group       haproxy
    daemon				//常见一个进程进入daemon模式运行

    # turn on stats unix socket		
    stats socket /var/lib/haproxy/stats
defaults
    mode                    http	//默认的模式mode{tcp|http|health}
    log                     global	//才用全局定义的日志
    option                  httplog	//日志类别:http日志格式
    option                  dontlognull	//不记录健康检查的日志信息
    option http-server-close
    option forwardfor       except 127.0.0.0/8	//后端服务器可以从Http Header中获得客户端ip
    option                  redispatch	//serverid服务器挂掉后强制定向到其他健康服务器
    retries                 3		//3次连接失败就认为服务不可用，也可以通过后面设置
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s		//如果backend没有指定，默认为10s
    timeout client          1m		//客户端连接超时
    timeout server          1m		//服务器连接超时
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000	//最大连接数
listen webs 0.0.0.0.:80
  balance roundrobin
  server user1 192.168.3.222:80
  server user2 192.168.3.244:80 check inter 2000 rise 2 fall 2

listen stats 0.0.0.0:1080	//监听端口
  stats uri /stats		//统计页面url
  stats refresh 30s		//统计页面自动刷新时间
  stats realm Haproxy Manager	//统计页面密码框上提示文本
  stats auth admin:admin	//统计页面用户名和密码设置

**3种调度其的对比
Nginx:web网站常用，支持正则，支持调度算法少:轮询，加权轮询
LVS:运行快，性能好，不支持正则，一般与keepalived一起使用，支持调度算法多
Haproxy:正则支持差，支持调度算法多
lvs可以实现客户端src直接连到web服务dest
nginx与haproxy则是客户端src连接到调度器后再最终访问到web服务dest
*Nginx分析
-优点:
1.工作在4层(应用层),可以针对http做分流策略
2.正则表达式比HaProxy强大
3.安装，配置，测试简单，可以通过日志解决多数问题
4.并发量可以达到几万次
5.Nginx还可以作为Web服务器使用
-缺点
1.仅支持http,https,mail协议，应用面小
2.监控检查仅通过端口，无法使用url检查
3.调度算法少，仅支持rr(轮询),wrr(加权轮询)
*LVS分析
-优点:
1.负载能力强，工作在4层，对内存,CPU消耗低
2.配置性低，没有太多可配置性，减少人为失误
3.应用面广，几乎可以为所有应用提供负载均衡
4.调度算法丰富
-缺点；
1.不支持正则表达式，不能实现动静分离
2.如果网站架构庞大，LVS-DR配置比较繁琐
*HAProxy分析
-优点:
1.支持session,cookie功能
2.可以通过url进行健康检查
3.效率，负载均衡速度高于Nginx
4.HAProxy支持TCP，可以对MYSQL进行负载均衡
5.调度算法丰富
-缺点:
1.正则弱于Nginx
2.日志依赖与syslogd,不支持apache日志


































